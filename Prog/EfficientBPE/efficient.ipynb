{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from collections import defaultdict, Counter\n",
    "import more_itertools\n",
    "import heapq\n",
    "from Bio import Align\n",
    "from Bio.Align import substitution_matrices\n",
    "import uuid\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load sequence data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../RSRC/uniref_taxonomy_id_9606_AND_identity_2024_09_13.json') as f:\n",
    "    human_proteins_json = json.load(f)['results']\n",
    "    \n",
    "human_proteins_df = []\n",
    "for prot in human_proteins_json:\n",
    "    human_proteins_df.append({'id': prot['id'], 'sequence': prot['representativeMember']['sequence']['value']})\n",
    "human_proteins_df = pd.DataFrame(human_proteins_df)\n",
    "human_proteins_df = human_proteins_df[~human_proteins_df['sequence'].str.contains('U')]\n",
    "\n",
    "df_ds_train, df_ds_test = train_test_split(human_proteins_df, test_size=0.2, random_state=42)\n",
    "\n",
    "corpus = df_ds_train['sequence']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the initial symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'X', 'Y']\n"
     ]
    }
   ],
   "source": [
    "alphabet = []\n",
    "for seq in corpus:\n",
    "    for letter in seq:\n",
    "        if letter not in alphabet:\n",
    "            alphabet.append(letter)\n",
    "alphabet.sort()\n",
    "print(alphabet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import custom classes for efficient BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_classes import Sym, SymList, SymPair, MaxHeapMap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert every protein sequence into a SymList (a doubly linked list containing tokens as nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = []\n",
    "for seq in corpus:\n",
    "    if len(seq) == 0: continue\n",
    "    symlist = SymList()\n",
    "    for sym_str in seq:\n",
    "        symlist.append(Sym(sym_str))\n",
    "    sequences.append(symlist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEVLRRSSVFAAEIMDAFDRCGDAADGLMSSSVWSAQTLASAPTGWWLHSAASAAS\n"
     ]
    }
   ],
   "source": [
    "print(sequences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct the initial MaxHeapMap\n",
    "\n",
    "This data structure maintains a max heap of SymbolPairs with respect to their occurence counts.\n",
    "- A SymbolPair is basically a struct that keeps track of every occurence of a token pair in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pair: (L, L), Count: 153309'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate the data structure for bookkeeping of all symbol pairs found in the data.\n",
    "\n",
    "# Maps (\"A\", \"B\") -> SymPair(\"A\", \"B\")\n",
    "pair_database = {}\n",
    "\n",
    "def add_entry(db, sym1, sym2):\n",
    "    sym1_str = sym1.literal\n",
    "    sym2_str = sym2.literal\n",
    "    curr_pair = db.get((sym1_str, sym2_str), None)\n",
    "    if curr_pair is None:\n",
    "        curr_pair = SymPair(sym1_str, sym2_str)\n",
    "        db[(sym1_str, sym2_str)] = curr_pair\n",
    "    curr_pair.add_pos((sym1, sym2))\n",
    "\n",
    "\n",
    "for seq in sequences:\n",
    "    for sym1, sym2 in more_itertools.pairwise(seq):\n",
    "        add_entry(pair_database, sym1, sym2)\n",
    "\n",
    "\n",
    "merge_heap = MaxHeapMap()\n",
    "for pair in pair_database.values():\n",
    "    merge_heap.push(pair)\n",
    "\n",
    "del pair_database\n",
    "\n",
    "str(merge_heap.peek())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[str(elem) for elem in merge_heap.heap[:10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the merging function.\n",
    "\n",
    "This function is the core of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_pair(sym_pair):\n",
    "    # print(f\"merging: {str(sym_pair)}\")\n",
    "    add_database = {}\n",
    "    remove_database = {}\n",
    "    for sym1, sym2 in sym_pair.positions:\n",
    "        \n",
    "        # Due to lazy removing, check if we are trying to merge an already removed pair\n",
    "        if sym1.next.literal != sym_pair.right: continue\n",
    "        if sym2.prev.literal != sym_pair.left: continue\n",
    "\n",
    "        merged_sym = Sym(sym1.literal + sym2.literal)\n",
    "        merged_sym.prev = sym1.prev\n",
    "        merged_sym.next = sym2.next\n",
    "        if sym1.prev is not None:\n",
    "            pre = sym1.prev\n",
    "            pre.next = merged_sym\n",
    "            # print(f\"Adding new entries to databases\", pre, merged_sym)\n",
    "            # print(f\"Adding new entries to databases\", pre, sym1)\n",
    "            add_entry(add_database, pre, merged_sym)\n",
    "            # Don't attempt to remove currently processed merge pair\n",
    "            if (pre.literal, sym1.literal) != (sym_pair.left, sym_pair.right): add_entry(remove_database, pre, sym1)\n",
    "        if sym2.next is not None:\n",
    "            nex = sym2.next\n",
    "            nex.prev = merged_sym\n",
    "            # print(f\"Adding new entries to databases\", pre, merged_sym)\n",
    "            # print(f\"Adding new entries to databases\", pre, sym1)\n",
    "            add_entry(add_database, merged_sym, nex)\n",
    "             # Don't attempt to remove currently processed merge pair\n",
    "            if (sym2.literal, nex.literal) != (sym_pair.left, sym_pair.right): add_entry(remove_database, sym2, nex)\n",
    "    # print(f\"Sizes of the new databases add and remove: {len(add_database)}, {len(remove_database)}\")\n",
    "    for val in add_database.values():\n",
    "        # print(f\"adding: {str(val)}\")\n",
    "        merge_heap.push(val)\n",
    "    for r_val in remove_database.values():\n",
    "        # print(f\"trying to remove: {str(r_val)}\")\n",
    "        inner_val = merge_heap.remove_by_value(r_val)\n",
    "        inner_val.count -= r_val.count\n",
    "        merge_heap.push(inner_val)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add words by vocabulary size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %%prun -s cumulative\n",
    "# init_vocab = alphabet.copy()\n",
    "\n",
    "# # Add 10000 words\n",
    "# for i in range(100):\n",
    "#     best_pair = merge_heap.pop()\n",
    "    \n",
    "#     merge_pair(best_pair)\n",
    "#     init_vocab.append(best_pair.merged())\n",
    "\n",
    "# print(init_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add words by cutoff frequency threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %%prun -s cumulative\n",
    "# init_vocab = alphabet.copy()\n",
    "\n",
    "# # Add 10000 words\n",
    "# best_pair = merge_heap.pop()\n",
    "# max_count = best_pair.count\n",
    "# while best_pair.count > max_count / 1000:\n",
    "#     merge_pair(best_pair)\n",
    "#     init_vocab.append(best_pair.merged())\n",
    "#     best_pair = merge_heap.pop()\n",
    "\n",
    "# print(init_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [print(str(k)) for k in merge_heap.map]\n",
    "# print(len(init_vocab))\n",
    "\n",
    "# cnts = Counter(init_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code blocks related to addition of mutated sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates a sorted list of mutations (above the cutoff similarity) for the given sequence.\n",
    "# Each resulting element is a tuple (m_sequence, score), where:\n",
    "# - m_sequence: the mutated sequence as string\n",
    "# - score: similarity score to the original sequence, a number between 0 and 1\n",
    "# Results are sorted from highest to lowest score.\n",
    "# First element of the results is the original sequence\n",
    "# Mutations are based only on substitutions with non-negative matrix values.\n",
    "def generate_mutations(seq, matrix, cutoff):\n",
    "    # if len(seq) > 10:\n",
    "    #     return []\n",
    "    alp = matrix.alphabet\n",
    "    candidates = []\n",
    "\n",
    "    max_score = 0\n",
    "    for aa in seq:\n",
    "        if aa != \"X\":\n",
    "            max_score += matrix[aa][aa]\n",
    "\n",
    "    # Create mutation candidates for each symbol in the sequence\n",
    "    for i, aa in enumerate(seq):\n",
    "        candidates.append([])\n",
    "        # Ignore X from calculation\n",
    "        if aa == \"X\":\n",
    "            candidates[i].append((aa, 0.0))\n",
    "            continue\n",
    "        # Consider substitutions with non-negative scores\n",
    "        for c_aa in alp:\n",
    "            score = matrix[aa][c_aa]\n",
    "            if score >= -1e-4: # for floating point precision\n",
    "                similarity_loss = (matrix[aa][aa] - score)/max_score\n",
    "                # if the similarity loss from this particular aminoacid is large enough\n",
    "                # to go under the cutoff, don't even consider it\n",
    "                if similarity_loss < 1 - cutoff:\n",
    "                    candidates[i].append((c_aa, similarity_loss))\n",
    "\n",
    "    for idx, candidate in enumerate(candidates):\n",
    "        candidates[idx] = sorted(candidate, key=lambda x: x[1])\n",
    "\n",
    "    # Use dfs to search mutations that has score above tbe cutoff, fast.\n",
    "    def dfs(current_seq, current_score, position):\n",
    "        # Prune branches where the score is under the cutoff\n",
    "        if 1-current_score < cutoff:\n",
    "            return\n",
    "        \n",
    "        # If the string is of the required length, save it\n",
    "        if position == len(seq):\n",
    "            final_mutations.append((current_seq, 1-current_score))\n",
    "            return\n",
    "        \n",
    "        # Try all candidate mutations at the current position\n",
    "        for candidate, candidate_score in candidates[position]:\n",
    "            # Prune if score is under the cutoff\n",
    "            # This pruning works because candidate scores are sorted\n",
    "            if 1-(current_score+candidate_score) < cutoff:\n",
    "                break\n",
    "            dfs(current_seq+candidate, current_score+candidate_score, position+1)\n",
    "\n",
    "    final_mutations = []\n",
    "    dfs(\"\", 0, 0)\n",
    "    return sorted(final_mutations, key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example usage of generate_mutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio.Align import substitution_matrices\n",
    "\n",
    "blosum62 = substitution_matrices.load(\"BLOSUM62\")\n",
    "pam250 = substitution_matrices.load(\"PAM250\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_mutations('HMVL', blosum62, 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example code block for also adding the mutated sequences during the training of BPE tokenizer\n",
    "\n",
    "Currently, it adds **all** the mutations that are generated from positive values in the substitution matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%prun -s cumulative\n",
    "# init_vocab = alphabet.copy()\n",
    "\n",
    "# Add 100 words + their mutations\n",
    "for i in range(1000):\n",
    "    best_pair = merge_heap.pop()\n",
    "    merge_pair(best_pair)\n",
    "    merged_string = best_pair.merged()\n",
    "    init_vocab.append(merged_string)\n",
    "\n",
    "    # For the mutations:\n",
    "    \n",
    "    # Consider only the mutations with a similarity score larger than 0.8\n",
    "    # [1:] ignores the original string\n",
    "    mutations = generate_mutations(merged_string, blosum62, 0.8)[1:]\n",
    "    for mutated_str, score in mutations:\n",
    "        pairs_to_merge = merge_heap.merged_to_pair.get(mutated_str, [])\n",
    "        if len(pairs_to_merge) > 0:\n",
    "            init_vocab.append(mutated_str)\n",
    "        for pair in pairs_to_merge:\n",
    "            merge_heap.remove_by_value(pair)\n",
    "            merge_pair(pair)\n",
    "\n",
    "print(init_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(init_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For debugging purposes, to see if any duplicates are added\n",
    "print(len(init_vocab))\n",
    "\n",
    "cnts = Counter(init_vocab)\n",
    "cnts\n",
    "\n",
    "# import json\n",
    "\n",
    "# with open(\"test.json\", \"w\") as f:\n",
    "#     json.dump(list(cnts.keys()), f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
