{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Tokenizer Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BPE pre 800',\n",
       " 'BPE pre 1600',\n",
       " 'BPE pre 3200',\n",
       " 'BPE pre 6400',\n",
       " 'BPE pre 12800',\n",
       " 'BPE pre 25600',\n",
       " 'BPE pre 51200',\n",
       " 'evoBPE pre pam70 0.7 0.05 800',\n",
       " 'evoBPE pre pam70 0.7 0.05 1600',\n",
       " 'evoBPE pre pam70 0.7 0.05 3200',\n",
       " 'evoBPE pre pam70 0.7 0.05 6400',\n",
       " 'evoBPE pre pam70 0.7 0.05 12800',\n",
       " 'evoBPE pre pam70 0.7 0.05 25600',\n",
       " 'evoBPE pre pam70 0.7 0.05 51200',\n",
       " 'evoBPE pre blosum62 0.7 0.05 800',\n",
       " 'evoBPE pre blosum62 0.7 0.05 1600',\n",
       " 'evoBPE pre blosum62 0.7 0.05 3200',\n",
       " 'evoBPE pre blosum62 0.7 0.05 6400',\n",
       " 'evoBPE pre blosum62 0.7 0.05 12800',\n",
       " 'evoBPE pre blosum62 0.7 0.05 25600',\n",
       " 'evoBPE pre blosum62 0.7 0.05 51200',\n",
       " 'BPE 800',\n",
       " 'BPE 1600',\n",
       " 'BPE 3200',\n",
       " 'BPE 6400',\n",
       " 'BPE 12800',\n",
       " 'BPE 25600',\n",
       " 'BPE 51200',\n",
       " 'evoBPE pam70 0.7 0.05 800',\n",
       " 'evoBPE pam70 0.7 0.05 1600',\n",
       " 'evoBPE pam70 0.7 0.05 3200',\n",
       " 'evoBPE pam70 0.7 0.05 6400',\n",
       " 'evoBPE pam70 0.7 0.05 12800',\n",
       " 'evoBPE pam70 0.7 0.05 25600',\n",
       " 'evoBPE pam70 0.7 0.05 51200',\n",
       " 'evoBPE blosum62 0.7 0.05 800',\n",
       " 'evoBPE blosum62 0.7 0.05 1600',\n",
       " 'evoBPE blosum62 0.7 0.05 3200',\n",
       " 'evoBPE blosum62 0.7 0.05 6400',\n",
       " 'evoBPE blosum62 0.7 0.05 12800',\n",
       " 'evoBPE blosum62 0.7 0.05 25600',\n",
       " 'evoBPE blosum62 0.7 0.05 51200']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from tokenizers import Tokenizer\n",
    "from itertools import product\n",
    "\n",
    "base_path = \"/cta/share/users/mutbpe/tokenizers/\"\n",
    "def generate_filepaths_labels(uniref_id, pret, subs, cut, minlen, maxlen, freq, vocab_sizes):\n",
    "    paths = []\n",
    "    labels = []\n",
    "    for id, pre, sub, c, ml, mxl, f, vs in product(uniref_id, pret, subs, cut, minlen, maxlen, freq, vocab_sizes):\n",
    "        if sub == \"std\":\n",
    "            paths.append(base_path + f\"blosum62/uniref{id}{\"pre\" if pre else \"\"}_bpe_{vs}.json\")\n",
    "            labels.append(f\"BPE{\" pre\" if pre else \"\"} {vs}\")\n",
    "        else:\n",
    "            paths.append(base_path + f\"{sub}/uniref{id}{\"pre\" if pre else \"\"}_mutbpe_{c}_{ml}_{mxl}_{f}_{vs}.json\")\n",
    "            labels.append(f\"evoBPE{\" pre\" if pre else \"\"} {sub} {c} {f} {vs}\")\n",
    "    return paths, labels\n",
    "\n",
    "vocab_sizes = [800, 1600, 3200, 6400, 12800, 25600, 51200]\n",
    "paths, labels = generate_filepaths_labels([50], [True, False], [\"std\", \"pam70\", \"blosum62\"], [0.7], [3], [12], [0.05], vocab_sizes)\n",
    "vocab_dict = {}\n",
    "tokenizer_dict = {}\n",
    "for path, label in zip(paths, labels):\n",
    "    with open(path) as f:\n",
    "        vocab_dict[label] = json.load(f)\n",
    "    idx = path.rfind(\"/\")\n",
    "    hf_path = path[:idx] + \"/hf_\" + path[idx+1:] \n",
    "    tokenizer_dict[label] = Tokenizer.from_file(hf_path)\n",
    "list(vocab_dict.keys())\n",
    "list(tokenizer_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70901\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import combinations\n",
    "import seaborn as sns\n",
    "\n",
    "db_file = \"/cta/share/users/uniprot/human/human.db\"\n",
    "conn = sqlite3.connect(db_file)\n",
    "df_uniprot_human_seqs = pd.read_sql(f\"SELECT Sequence FROM proteins WHERE Entry IN (SELECT uniprot_accession FROM uniref50_distilled)\", conn)\n",
    "conn.close()\n",
    "# filtered_sequences = df_uniprot_human_seqs[\n",
    "#     (df_uniprot_human_seqs[\"Sequence\"].str.count(\"X\") <= 1) &\n",
    "#     (df_uniprot_human_seqs[\"Sequence\"].str.count(\"B\") <= 1) &\n",
    "#     (df_uniprot_human_seqs[\"Sequence\"].str.count(\"U\") <= 1) &\n",
    "#     (df_uniprot_human_seqs[\"Sequence\"].str.count(\"Z\") <= 1)\n",
    "# ][\"Sequence\"].tolist()\n",
    "filtered_sequences = df_uniprot_human_seqs[\"Sequence\"].tolist()\n",
    "print(len(filtered_sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "236550\n"
     ]
    }
   ],
   "source": [
    "db_file = \"/cta/share/users/uniprot/human/human.db\"\n",
    "conn = sqlite3.connect(db_file)\n",
    "uniref_id = 50\n",
    "\n",
    "df_uniprot_human_seqs = pd.read_sql(f\"SELECT * FROM uniref{uniref_id}_domain_sliced_plddt70\", conn)\n",
    "df_protein = pd.read_sql(f\"\"\"SELECT Entry as uniprot_id, Sequence as sequence\n",
    "                        FROM proteins\n",
    "                        WHERE Entry IN (SELECT uniprot_accession FROM uniref{uniref_id}_distilled)\"\"\", conn)\n",
    "\n",
    "conn.close()\n",
    "\n",
    "df_uniprot_human_seqs = df_uniprot_human_seqs[~df_uniprot_human_seqs['uniprot_id'].isin(df_protein[df_protein['sequence'].str.len() > 3000]['uniprot_id'].unique())]\n",
    "\n",
    "filtered_sequences2 = df_uniprot_human_seqs[\n",
    "    (df_uniprot_human_seqs[\"sequence\"].str.count(\"X\") <= 1) &\n",
    "    (df_uniprot_human_seqs[\"sequence\"].str.count(\"B\") <= 1) &\n",
    "    (df_uniprot_human_seqs[\"sequence\"].str.count(\"U\") <= 1) &\n",
    "    (df_uniprot_human_seqs[\"sequence\"].str.count(\"Z\") <= 1)\n",
    "][\"sequence\"].tolist()\n",
    "print(len(filtered_sequences2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BPE pre 800',\n",
       " 'BPE pre 6400',\n",
       " 'BPE pre 25600',\n",
       " 'evoBPE pre pam70 0.7 0.05 800',\n",
       " 'evoBPE pre pam70 0.7 0.05 6400',\n",
       " 'evoBPE pre pam70 0.7 0.05 25600',\n",
       " 'evoBPE pre blosum62 0.7 0.05 800',\n",
       " 'evoBPE pre blosum62 0.7 0.05 6400',\n",
       " 'evoBPE pre blosum62 0.7 0.05 25600',\n",
       " 'BPE 800',\n",
       " 'BPE 6400',\n",
       " 'BPE 25600',\n",
       " 'evoBPE pam70 0.7 0.05 800',\n",
       " 'evoBPE pam70 0.7 0.05 6400',\n",
       " 'evoBPE pam70 0.7 0.05 25600',\n",
       " 'evoBPE blosum62 0.7 0.05 800',\n",
       " 'evoBPE blosum62 0.7 0.05 6400',\n",
       " 'evoBPE blosum62 0.7 0.05 25600']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_vocabs = [\"800\", \"6400\", \"25600\"]\n",
    "encoded_dict = {}\n",
    "for k, v in tokenizer_dict.items():\n",
    "    if k.split()[-1] in test_vocabs:\n",
    "        if \"pre\" in k:\n",
    "            encoded_dict[k] = [enc.tokens for enc in v.encode_batch(filtered_sequences2)]\n",
    "        else:\n",
    "            encoded_dict[k] = [enc.tokens for enc in v.encode_batch(filtered_sequences)]\n",
    "\n",
    "list(encoded_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method|pretokenized|s_matrix|v_size|p_ratio|p_ratio*|m_ratio|m_ratio*|avg_t_length|avg_t_length*\n",
      "['BPE', 'pre', '800']|Yes|-|800|1.0|1.0|0.0|0.0|2.513 ± 0.594|1.937 ± 0.562\n",
      "['BPE', 'pre', '6400']|Yes|-|6400|1.0|1.0|0.0|0.0|3.4 ± 0.711|2.438 ± 0.711\n",
      "['BPE', 'pre', '25600']|Yes|-|25600|1.0|1.0|0.0|0.0|3.891 ± 1.185|2.815 ± 0.912\n",
      "['evoBPE', 'pre', 'pam70', '0.7', '0.05', '800']|Yes|PAM70|800|0.081|0.034|0.487|0.077|2.591 ± 0.646|1.891 ± 0.577\n",
      "['evoBPE', 'pre', 'pam70', '0.7', '0.05', '6400']|Yes|PAM70|6400|0.145|0.117|0.787|0.267|3.347 ± 0.679|2.39 ± 0.679\n",
      "['evoBPE', 'pre', 'pam70', '0.7', '0.05', '25600']|Yes|PAM70|25600|0.114|0.158|0.866|0.403|3.894 ± 0.952|2.737 ± 0.863\n",
      "['evoBPE', 'pre', 'blosum62', '0.7', '0.05', '800']|Yes|BLOSUM62|800|0.062|0.029|0.524|0.085|2.6 ± 0.627|1.884 ± 0.585\n",
      "['evoBPE', 'pre', 'blosum62', '0.7', '0.05', '6400']|Yes|BLOSUM62|6400|0.103|0.098|0.836|0.293|3.334 ± 0.654|2.386 ± 0.678\n",
      "['evoBPE', 'pre', 'blosum62', '0.7', '0.05', '25600']|Yes|BLOSUM62|25600|0.074|0.128|0.909|0.439|3.879 ± 0.937|2.733 ± 0.858\n"
     ]
    }
   ],
   "source": [
    "import vocabulary_functions as vf\n",
    "from collections import Counter\n",
    "from statistics import mean, stdev\n",
    "print(f\"Method|pretokenized|s_matrix|v_size|p_ratio|p_ratio*|m_ratio|m_ratio*|avg_t_length|avg_t_length*\")\n",
    "for k in encoded_dict:\n",
    "    cur_vocab = vocab_dict[k]\n",
    "    if \"evo\" not in k:\n",
    "        parents = cur_vocab\n",
    "    else:\n",
    "        parents = vf.get_parents(cur_vocab)\n",
    "    mutateds = vf.get_mutated(cur_vocab)\n",
    "    parent_ratio = round(len(parents)/len(cur_vocab), 3)\n",
    "    mutated_ratio = round(len(mutateds)/len(cur_vocab), 3)\n",
    "\n",
    "    token_lengths = [len(k) for k in cur_vocab]\n",
    "    avg_token_length = round(mean(token_lengths), 3)\n",
    "    std_token_length = round(stdev(token_lengths), 3)\n",
    "    \n",
    "    cur_encodings = encoded_dict[k]\n",
    "    all_tokens = []\n",
    "    for enc in cur_encodings:\n",
    "        for t in enc:\n",
    "            all_tokens.append(t)\n",
    "    used_token_counts = Counter(all_tokens)\n",
    "    used_parent_counts = vf.set_intersection(used_token_counts, parents)\n",
    "    used_mutated_counts = vf.set_intersection(used_token_counts, mutateds)\n",
    "\n",
    "    used_token_lengths = [len(k) for k in all_tokens]\n",
    "    avg_used_token_length = round(mean(used_token_lengths), 3)\n",
    "    std_used_token_length = round(stdev(used_token_lengths), 3)\n",
    "\n",
    "    used_parent_ratio = round(sum(used_parent_counts.values())/len(all_tokens), 3)\n",
    "    used_mutated_ratio = round(sum(used_mutated_counts.values())/len(all_tokens), 3)\n",
    "    \n",
    "    pret = \"Yes\" if \"pre\" in k else \"No\"\n",
    "    name = k.split()[0]\n",
    "    if \"evo\" in k:\n",
    "        matrix = k.split()[2] if \"pre\" in k else k.split()[1]\n",
    "    else:\n",
    "        matrix = \"-\"\n",
    "    vsize = k.split()[-1]\n",
    "\n",
    "    out_str = f\"{name}|{pret}|{matrix.upper()}|{vsize}|{parent_ratio}|{used_parent_ratio}|{mutated_ratio}|{used_mutated_ratio}|{avg_token_length} ± {std_token_length}|{avg_used_token_length} ± {std_used_token_length}\"\n",
    "    print(out_str)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
