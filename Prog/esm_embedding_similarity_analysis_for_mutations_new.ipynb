{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ESM-2 Embedding Similarity Analysis for Mutations\n",
    "\n",
    "<img src=\"../figures/esm_mutation.svg\" alt=\"ESM-2 Embedding Similarity Analysis for Mutations\" width=\"350px\">\n",
    "\n",
    "ESM-2 is one of the most advanced protein language models out there. It’s trained on millions of protein sequences using self-supervised learning, meaning it learns patterns and relationships without needing labeled data. What makes it special is that its embeddings—basically, how it represents proteins—capture both their structure and evolutionary history. That’s why it performs so well in tasks like predicting protein structures and figuring out their functions.\n",
    "\n",
    "For our experiment, we used the 650-million parameter version of ESM-2 to test how well mutation-based token replacements in evoBPE preserve biological meaning. In simple terms, we wanted to see if swapping amino acids in a way that mimics real mutations keeps the original protein’s properties better than just making random substitutions. Even though mutation tokens are relatively rare in evoBPE’s training data, we wanted to check if they still lead to more biologically meaningful changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"HF_HOME\"] = \"/cta/share/users/esm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from transformers import EsmTokenizer, EsmModel, EsmForMaskedLM\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from pandarallel import pandarallel\n",
    "from Bio.Align import substitution_matrices\n",
    "import ast\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "import random\n",
    "from scipy import stats\n",
    "\n",
    "from vocabulary_functions import get_mutated, get_parents, set_difference, set_intersection, load_tokenizers, calc_agreement, calc_dice_idx_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 20 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "pandarallel.initialize(progress_bar=True, nb_workers=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to DB\n",
    "db_file = \"/cta/share/users/uniprot/human/human.db\"\n",
    "conn = sqlite3.connect(db_file)\n",
    "\n",
    "uniref_id = '50'\n",
    "df_protein = pd.read_sql(f\"\"\"SELECT Entry as uniprot_id, Sequence as sequence\n",
    "                          FROM proteins\n",
    "                          WHERE Entry IN (SELECT uniprot_accession FROM uniref{uniref_id}_distilled)\"\"\", conn)\n",
    "df_protein = df_protein[df_protein['sequence'].str.len() < 512].reset_index(drop=True)\n",
    "\n",
    "df_protein_sliced = pd.read_sql(f\"SELECT uniprot_id, sequence FROM uniref{uniref_id}_domain_sliced_plddt70\", conn)\n",
    "df_protein_sliced = df_protein_sliced[df_protein_sliced['uniprot_id'].isin(df_protein['uniprot_id'])].reset_index(drop=True)\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'dataset': {'uniref50', 'uniref90'}\n",
    "# 'is_pretokenizer': {True, False}\n",
    "# 'subs_matrix': {'blosum45', 'blosum62', 'pam70', 'pam250'}\n",
    "# 'mutation_cutoff': {0.7, 0.8, 0.9}\n",
    "# 'min_mutation_freq': {0, 0.05,. 0.005}\n",
    "# 'min_mutation_len': {3}\n",
    "# 'max_mutation_len': {12}\n",
    "# 'vocab_size': list=[800, 1600, 3200, 6400, 12800, 25600, 51200]\n",
    "\n",
    "vocab_sizes = [51200]\n",
    "uniref_id = \"50\"\n",
    "\n",
    "tokenizer_opts_list = [\n",
    "    {\n",
    "        'is_mut': True,\n",
    "        'dataset': f'uniref{uniref_id}',\n",
    "        'is_pretokenizer': False,\n",
    "        'subs_matrix': 'blosum62',\n",
    "        'mutation_cutoff': 0.7,\n",
    "        'min_mutation_freq': 0.05,\n",
    "        'min_mutation_len': 3,\n",
    "        'max_mutation_len': 12,\n",
    "        'vocab_size': vocab_sizes\n",
    "    },\n",
    "    {\n",
    "        'is_mut': True,\n",
    "        'dataset': f'uniref{uniref_id}',\n",
    "        'is_pretokenizer': False,\n",
    "        'subs_matrix': 'blosum62',\n",
    "        'mutation_cutoff': 0.7,\n",
    "        'min_mutation_freq': 0.005,\n",
    "        'min_mutation_len': 3,\n",
    "        'max_mutation_len': 12,\n",
    "        'vocab_size': vocab_sizes\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "tokenizer_list = load_tokenizers(tokenizer_opts_list, 'hf')\n",
    "tokenizer_list['PUMA blosum62 0.7 0.005 51200'] = Tokenizer.from_file(f\"/cta/share/users/mutbpe/tokenizers/blosum62_alldataset/1000000/hf_uniref50_mutbpe_0.7_3_12_0.05_51200.json\")\n",
    "\n",
    "inner_vocab_list = load_tokenizers(tokenizer_opts_list, 'vocab')\n",
    "with open(f\"/cta/share/users/mutbpe/tokenizers/blosum62_alldataset/1000000/uniref50_mutbpe_0.7_3_12_0.05_51200.json\") as json_file:\n",
    "    inner_vocab_list['PUMA blosum62 0.7 0.005 51200'] = json.load(json_file)\n",
    "\n",
    "vocab_list = {name: list(set(tok.get_vocab().keys())) for name, tok in tokenizer_list.items()}\n",
    "methods = [name[:-len(str(vocab_sizes[0]))-1] for name in list(tokenizer_list.keys())[::len(vocab_sizes)]]\n",
    "methods2names = {mn: mn.replace('mut', 'evo').replace('std', '').replace('blosum', 'BLOSUM').replace('pam', 'PAM').replace('pre', 'Pre') for mn in methods}\n",
    "methods2names = {k: ' '.join(v.split()[:-2]) if 'evoBPE' in v else v for k, v in methods2names.items()}\n",
    "\n",
    "inner_vocab_parents_list = {}\n",
    "inner_vocab_mutated_list = {}\n",
    "for k, v in inner_vocab_list.items():\n",
    "    inner_vocab_parents_list[k] = get_parents(v)\n",
    "    inner_vocab_mutated_list[k] = get_mutated(v)\n",
    "\n",
    "for tokenizer_name in tokenizer_list.keys():\n",
    "    for mutated_token, mutated_token_attr in inner_vocab_mutated_list[tokenizer_name].items():\n",
    "        parent_token = mutated_token_attr['parent']\n",
    "        inner_vocab_parents_list[tokenizer_name][parent_token]['mutations'] = inner_vocab_parents_list[tokenizer_name][parent_token].get('mutations', []) + [mutated_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building Vocabulary Lineage: 100%|██████████| 2/2 [00:00<00:00, 13.46it/s]\n"
     ]
    }
   ],
   "source": [
    "# --- Vocabulary Lineage Construction ---\n",
    "vocab_lineage_list = {}\n",
    "for k, v in inner_vocab_list.items():\n",
    "    vocab_lineage_list[k] = {token: {\n",
    "        'frequency': -1, 'order': -1, 'parent_pair': [], 'parent_mutation': \"\",\n",
    "        'parent_mutation_similarity': -1, 'partner_pair_self': False,\n",
    "        'partner_pair_left': [], 'partner_pair_right': [], 'child_pair': [], 'child_mutation': []\n",
    "    } for token in v.keys()}\n",
    "\n",
    "for method_name, vocab in tqdm(inner_vocab_list.items(), desc=\"Building Vocabulary Lineage\"):\n",
    "    for token, inner_elements in vocab.items():\n",
    "        lineage = vocab_lineage_list[method_name][token]\n",
    "        lineage['frequency'] = inner_elements.get('frequency', -1)\n",
    "        lineage['order'] = inner_elements.get('order', -1)\n",
    "        lineage['parent_pair'] = inner_elements.get('pair', [])\n",
    "        lineage['parent_mutation'] = inner_elements.get('parent', \"\")\n",
    "        lineage['parent_mutation_similarity'] = inner_elements.get('similarity', -1)\n",
    "\n",
    "        if 'pair' in inner_elements:\n",
    "            p1, p2 = inner_elements['pair']\n",
    "            if p1 == p2:\n",
    "                vocab_lineage_list[method_name][p1]['partner_pair_self'] = True\n",
    "                vocab_lineage_list[method_name][p1]['child_pair'].append(token)\n",
    "            else:\n",
    "                vocab_lineage_list[method_name][p1]['partner_pair_right'].append(p2)\n",
    "                vocab_lineage_list[method_name][p2]['partner_pair_left'].append(p1)\n",
    "                vocab_lineage_list[method_name][p1]['child_pair'].append(token)\n",
    "                vocab_lineage_list[method_name][p2]['child_pair'].append(token)\n",
    "        if 'parent' in inner_elements:\n",
    "            parent = inner_elements['parent']\n",
    "            vocab_lineage_list[method_name][parent]['child_mutation'].append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.44it/s]\n"
     ]
    }
   ],
   "source": [
    "for name, tokenizer in tqdm(list(tokenizer_list.items())):\n",
    "    if 'pre' in name:\n",
    "        df_protein_sliced[name] = [enc.tokens for enc in tokenizer.encode_batch(df_protein_sliced['sequence'])]\n",
    "    else:\n",
    "        df_protein[name] = [enc.tokens for enc in tokenizer.encode_batch(df_protein['sequence'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uniprot_id</th>\n",
       "      <th>sequence</th>\n",
       "      <th>PUMA blosum62 0.7 0.05 51200</th>\n",
       "      <th>PUMA blosum62 0.7 0.005 51200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A0A087WZT3</td>\n",
       "      <td>MELSAEYLREKLQRDLEAEHVLPSPGGVGQVRGETAASETQLGS</td>\n",
       "      <td>[MEL, SA, EYL, REKL, QRDL, EAEH, VL, PSP, GGVG...</td>\n",
       "      <td>[ME, LSAE, YLR, EKLQ, RDLE, AEHV, LPSP, GGVG, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A0A0B4J2F0</td>\n",
       "      <td>MFRRLTFAQLLFATVLGIAGGVYIFQPVFEQYAKDQKELKEKMQLV...</td>\n",
       "      <td>[M, FRRL, TFA, QLL, FAT, VLG, IA, GGV, YI, FQ,...</td>\n",
       "      <td>[MFRR, LTF, AQ, LLF, AT, VLGI, AGGV, YI, FQ, P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A0A0C5B5G6</td>\n",
       "      <td>MRWQEMGYIFYPRKLR</td>\n",
       "      <td>[MRW, QEMG, YI, FY, PRKL, R]</td>\n",
       "      <td>[MRW, QE, MG, YI, FYP, RKLR]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A0A0K2S4Q6</td>\n",
       "      <td>MTQRAGAAMLPSALLLLCVPGCLTVSGPSTVMGAVGESLSVQCRYE...</td>\n",
       "      <td>[M, TQR, AGAA, ML, PSA, LLLL, CV, PGCL, TVSG, ...</td>\n",
       "      <td>[MTQ, RAG, AAM, LP, SALL, LLCV, PGC, LTV, SGP,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A0A0S2Z4G9</td>\n",
       "      <td>MNQSRSRSDGGSEETLPQDHNHHENERRWQQERLHREEAYYQFINE...</td>\n",
       "      <td>[MNQ, SRSR, SDGG, SEE, TLPQ, DH, NHH, ENERR, W...</td>\n",
       "      <td>[MNQ, SRSR, SDGG, SEE, TLP, QD, HN, HHE, NE, R...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   uniprot_id                                           sequence  \\\n",
       "0  A0A087WZT3       MELSAEYLREKLQRDLEAEHVLPSPGGVGQVRGETAASETQLGS   \n",
       "1  A0A0B4J2F0  MFRRLTFAQLLFATVLGIAGGVYIFQPVFEQYAKDQKELKEKMQLV...   \n",
       "2  A0A0C5B5G6                                   MRWQEMGYIFYPRKLR   \n",
       "3  A0A0K2S4Q6  MTQRAGAAMLPSALLLLCVPGCLTVSGPSTVMGAVGESLSVQCRYE...   \n",
       "4  A0A0S2Z4G9  MNQSRSRSDGGSEETLPQDHNHHENERRWQQERLHREEAYYQFINE...   \n",
       "\n",
       "                        PUMA blosum62 0.7 0.05 51200  \\\n",
       "0  [MEL, SA, EYL, REKL, QRDL, EAEH, VL, PSP, GGVG...   \n",
       "1  [M, FRRL, TFA, QLL, FAT, VLG, IA, GGV, YI, FQ,...   \n",
       "2                       [MRW, QEMG, YI, FY, PRKL, R]   \n",
       "3  [M, TQR, AGAA, ML, PSA, LLLL, CV, PGCL, TVSG, ...   \n",
       "4  [MNQ, SRSR, SDGG, SEE, TLPQ, DH, NHH, ENERR, W...   \n",
       "\n",
       "                       PUMA blosum62 0.7 0.005 51200  \n",
       "0  [ME, LSAE, YLR, EKLQ, RDLE, AEHV, LPSP, GGVG, ...  \n",
       "1  [MFRR, LTF, AQ, LLF, AT, VLGI, AGGV, YI, FQ, P...  \n",
       "2                       [MRW, QE, MG, YI, FYP, RKLR]  \n",
       "3  [MTQ, RAG, AAM, LP, SALL, LLCV, PGC, LTV, SGP,...  \n",
       "4  [MNQ, SRSR, SDGG, SEE, TLP, QD, HN, HHE, NE, R...  "
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_protein_sliced = df_protein_sliced.groupby('uniprot_id').sum().reset_index()\n",
    "df_protein = df_protein.set_index(['uniprot_id', 'sequence']).join(df_protein_sliced.set_index(['uniprot_id', 'sequence'])).reset_index()\n",
    "df_protein.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Pre-computation Function ---\n",
    "\n",
    "def precompute_alternatives(sub_matrix):\n",
    "    \"\"\"\n",
    "    Pre-computes a lookup table for amino acid substitutions with better scores.\n",
    "    \"\"\"\n",
    "    print(\"Pre-computing alternative amino acid scores...\")\n",
    "    AMINO_ACIDS = sub_matrix.alphabet[:-1]\n",
    "    precomputed_alternatives = {orig_aa: {} for orig_aa in AMINO_ACIDS}\n",
    "\n",
    "    for orig_aa in tqdm(AMINO_ACIDS):\n",
    "        for mut_aa in AMINO_ACIDS:\n",
    "            if orig_aa == mut_aa:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                score_mutation = sub_matrix[(orig_aa, mut_aa)]\n",
    "            except KeyError:\n",
    "                continue\n",
    "\n",
    "            possible_alternatives = []\n",
    "            for alt_aa in AMINO_ACIDS:\n",
    "                if alt_aa in (orig_aa, mut_aa):\n",
    "                    continue\n",
    "                try:\n",
    "                    score_alternative = sub_matrix[(orig_aa, alt_aa)]\n",
    "                    if score_alternative == score_mutation:\n",
    "                        possible_alternatives.append((alt_aa, score_alternative))\n",
    "                except KeyError:\n",
    "                    continue\n",
    "            \n",
    "            # Sort by score (descending) and store only the amino acid\n",
    "            possible_alternatives.sort(key=lambda x: x[1], reverse=True)\n",
    "            precomputed_alternatives[orig_aa][mut_aa] = [alt[0] for alt in possible_alternatives]\n",
    "            \n",
    "    return precomputed_alternatives\n",
    "\n",
    "\n",
    "# --- Enhanced Helper Functions ---\n",
    "\n",
    "def find_alternative_replacement_optimized(\n",
    "    original_token, \n",
    "    mutation_replacement, \n",
    "    replacement_pool_set, \n",
    "    precomputed_alts,\n",
    "    vocab_lineage\n",
    "):\n",
    "    \"\"\"\n",
    "    Finds an alternative token using the pre-computed lookup table.\n",
    "    \"\"\"\n",
    "    if len(original_token) != len(mutation_replacement):\n",
    "        return mutation_replacement\n",
    "\n",
    "    diff_positions = [\n",
    "        i for i, (orig_aa, mut_aa) in enumerate(zip(original_token, mutation_replacement)) \n",
    "        if orig_aa != mut_aa\n",
    "    ]\n",
    "\n",
    "    if not diff_positions:\n",
    "        return mutation_replacement\n",
    "\n",
    "    alternative_token = mutation_replacement\n",
    "\n",
    "    for pos in diff_positions:\n",
    "        original_aa = original_token[pos]\n",
    "        mutated_aa = mutation_replacement[pos]\n",
    "\n",
    "        best_alternatives = precomputed_alts.get(original_aa, {}).get(mutated_aa, [])\n",
    "        \n",
    "        for alt_aa in best_alternatives:\n",
    "            alt_token_list = list(alternative_token)\n",
    "            alt_token_list[pos] = alt_aa\n",
    "            new_token = \"\".join(alt_token_list)\n",
    "            \n",
    "            if (new_token not in replacement_pool_set) and (new_token not in vocab_lineage):\n",
    "                alternative_token = new_token\n",
    "                break\n",
    "\n",
    "    return alternative_token\n",
    "\n",
    "\n",
    "def create_random_alternative_baseline(original_token, mutation_replacement, sub_matrix):\n",
    "    \"\"\"\n",
    "    Create random alternative replacements matched for BLOSUM scores.\n",
    "    \"\"\"\n",
    "    if len(original_token) != len(mutation_replacement):\n",
    "        return mutation_replacement\n",
    "    \n",
    "    AMINO_ACIDS = list('ABCDEFGHIKLMNPQRSTVWYZ')\n",
    "    alternative_token = list(mutation_replacement)\n",
    "    \n",
    "    for i, (orig_aa, mut_aa) in enumerate(zip(original_token, mutation_replacement)):\n",
    "        if orig_aa != mut_aa:\n",
    "            try:\n",
    "                target_score = sub_matrix[(orig_aa, mut_aa)]\n",
    "                # Find amino acids with similar BLOSUM scores\n",
    "                candidates = []\n",
    "                for aa in AMINO_ACIDS:\n",
    "                    if aa != orig_aa and aa != mut_aa:\n",
    "                        try:\n",
    "                            score = sub_matrix[(orig_aa, aa)]\n",
    "                            if abs(score - target_score) <= 1:  # Allow ±1 score difference\n",
    "                                candidates.append(aa)\n",
    "                        except KeyError:\n",
    "                            continue\n",
    "                \n",
    "                if candidates:\n",
    "                    alternative_token[i] = random.choice(candidates)\n",
    "            except KeyError:\n",
    "                continue\n",
    "    \n",
    "    return ''.join(alternative_token)\n",
    "\n",
    "\n",
    "# --- Enhanced Main Function ---\n",
    "\n",
    "def run_mutation_experiment_optimized(df_protein, vocab_lineage_list, sub_matrix_precomputed_alternatives, create_baseline=True):\n",
    "    \"\"\"\n",
    "    Generates mutated, alternative, and baseline sequences efficiently.\n",
    "    \"\"\"\n",
    "    df_results = df_protein.copy()\n",
    "    token_cols = list(tokenizer_list.keys())\n",
    "    change_counters = {}\n",
    "\n",
    "    for col_name in token_cols:\n",
    "        print(f\"\\nProcessing column: {col_name}\")\n",
    "        change_counters[col_name] = 0\n",
    "\n",
    "        precomputed_alternatives = sub_matrix_precomputed_alternatives[col_name.split()[1]]\n",
    "        sub_matrix = substitution_matrices.load(col_name.split()[1].upper())\n",
    "        \n",
    "        all_mutated_sequences = []\n",
    "        all_alternative_sequences = []\n",
    "        all_baseline_sequences = [] if create_baseline else None\n",
    "        \n",
    "        vocab_lineage = vocab_lineage_list.get(col_name, {})\n",
    "\n",
    "        for tokenized_sequence in tqdm(df_results[col_name]):\n",
    "            new_mutated_sequence = []\n",
    "            new_alternative_sequence = []\n",
    "            new_baseline_sequence = [] if create_baseline else None\n",
    "            \n",
    "            for token in tokenized_sequence:\n",
    "                token_info = vocab_lineage.get(token)\n",
    "                \n",
    "                if not token_info:\n",
    "                    new_mutated_sequence.append(token)\n",
    "                    new_alternative_sequence.append(token)\n",
    "                    if create_baseline:\n",
    "                        new_baseline_sequence.append(token)\n",
    "                    continue\n",
    "                \n",
    "                if 'child_mutation' in token_info and token_info['child_mutation']:\n",
    "                    replacement_pool = token_info.get('child_mutation', [])\n",
    "                else:\n",
    "                    new_mutated_sequence.append(token)\n",
    "                    new_alternative_sequence.append(token)\n",
    "                    if create_baseline:\n",
    "                        new_baseline_sequence.append(token)\n",
    "                    continue\n",
    "\n",
    "                replacement_pool_set = set(replacement_pool + [token])\n",
    "\n",
    "                for mutation_replacement in replacement_pool:\n",
    "                    alternative_replacement = find_alternative_replacement_optimized(\n",
    "                        token, \n",
    "                        mutation_replacement, \n",
    "                        replacement_pool_set, \n",
    "                        precomputed_alternatives,\n",
    "                        vocab_lineage\n",
    "                    )\n",
    "                    \n",
    "                    if alternative_replacement != mutation_replacement:\n",
    "                        new_mutated_sequence.append(mutation_replacement)\n",
    "                        new_alternative_sequence.append(alternative_replacement)\n",
    "                        if create_baseline:\n",
    "                            baseline_replacement = create_random_alternative_baseline(\n",
    "                                token, mutation_replacement, sub_matrix\n",
    "                            )\n",
    "                            new_baseline_sequence.append(baseline_replacement)\n",
    "                        change_counters[col_name] += 1\n",
    "                        break\n",
    "                else:\n",
    "                    new_mutated_sequence.append(token)\n",
    "                    new_alternative_sequence.append(token)\n",
    "                    if create_baseline:\n",
    "                        new_baseline_sequence.append(token)\n",
    "\n",
    "            all_mutated_sequences.append(new_mutated_sequence)\n",
    "            all_alternative_sequences.append(new_alternative_sequence)\n",
    "            if create_baseline:\n",
    "                all_baseline_sequences.append(new_baseline_sequence)\n",
    "\n",
    "        df_results[f'{col_name} mutated'] = all_mutated_sequences\n",
    "        df_results[f'{col_name} alternative'] = all_alternative_sequences\n",
    "        if create_baseline:\n",
    "            df_results[f'{col_name} baseline'] = all_baseline_sequences\n",
    "        \n",
    "    return df_results, change_counters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Enhanced ESM-2 Functions ---\n",
    "\n",
    "def get_esm2_model_and_tokenizer(model_name=\"facebook/esm2_t30_150M_UR50D\"):\n",
    "    \"\"\"Initialize ESM-2 model and tokenizer.\"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    tokenizer = EsmTokenizer.from_pretrained(model_name)\n",
    "    model = EsmForMaskedLM.from_pretrained(model_name).to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"Loaded ESM-2 model: {model_name}\")\n",
    "    return model, tokenizer, device\n",
    "\n",
    "\n",
    "def get_esm2_embeddings_and_logits_batch(sequences, model, tokenizer, device, batch_size=32, max_length=1024, extract_layers=None):\n",
    "    \"\"\"\n",
    "    Get ESM-2 embeddings and logits for multiple protein sequences in batches.\n",
    "    \n",
    "    Args:\n",
    "        extract_layers: List of layer indices to extract embeddings from (default: last layer only)\n",
    "    \"\"\"\n",
    "    if extract_layers is None:\n",
    "        extract_layers = [-1]  # Last layer only\n",
    "    \n",
    "    sequence_data = {}\n",
    "    \n",
    "    for i in tqdm(range(0, len(sequences), batch_size), desc=\"Processing embedding batches\"):\n",
    "        batch_sequences = sequences[i:i + batch_size]\n",
    "        \n",
    "        inputs = tokenizer(\n",
    "            batch_sequences, \n",
    "            return_tensors=\"pt\", \n",
    "            padding='longest', \n",
    "            truncation=True, \n",
    "            max_length=max_length\n",
    "        ).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs, output_hidden_states=True)\n",
    "            batch_logits = outputs.logits\n",
    "            hidden_states = outputs.hidden_states\n",
    "        \n",
    "        for j, seq in enumerate(batch_sequences):\n",
    "            seq_len = len(seq)\n",
    "            \n",
    "            # Store logits (for probability analysis)\n",
    "            logits = batch_logits[j, 1:seq_len+1].cpu()  # Remove <cls>, keep actual sequence\n",
    "            \n",
    "            # Store embeddings from specified layers\n",
    "            embeddings = {}\n",
    "            for layer_idx in extract_layers:\n",
    "                layer_embeddings = hidden_states[layer_idx][j, 1:seq_len+1].cpu()\n",
    "                embeddings[layer_idx] = layer_embeddings\n",
    "            \n",
    "            sequence_data[seq] = {\n",
    "                'logits': logits,\n",
    "                'embeddings': embeddings\n",
    "            }\n",
    "    \n",
    "    return sequence_data\n",
    "\n",
    "\n",
    "def compute_masked_probabilities(sequence, changed_positions, model, tokenizer, device):\n",
    "    \"\"\"\n",
    "    Compute masked probabilities for specific positions in a sequence.\n",
    "    \"\"\"\n",
    "    probabilities = {}\n",
    "    \n",
    "    for pos in changed_positions:\n",
    "        # Create masked sequence\n",
    "        seq_list = list(sequence)\n",
    "        original_aa = seq_list[pos]\n",
    "        seq_list[pos] = tokenizer.mask_token\n",
    "        masked_seq = ''.join(seq_list)\n",
    "        \n",
    "        # Tokenize and get predictions\n",
    "        inputs = tokenizer(masked_seq, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "        \n",
    "        # Get probabilities at the masked position\n",
    "        mask_token_index = (inputs.input_ids == tokenizer.mask_token_id).nonzero(as_tuple=True)[1]\n",
    "        if len(mask_token_index) > 0:\n",
    "            mask_pos = mask_token_index[0]\n",
    "            position_logits = logits[0, mask_pos, :]\n",
    "            position_probs = F.softmax(position_logits, dim=-1)\n",
    "            \n",
    "            # Get probabilities for all amino acids\n",
    "            aa_probs = {}\n",
    "            for aa in 'ABCDEFGHIKLMNPQRSTVWYZ':\n",
    "                aa_token_id = tokenizer.convert_tokens_to_ids(aa)\n",
    "                if aa_token_id is not None:\n",
    "                    aa_probs[aa] = position_probs[aa_token_id].item()\n",
    "            \n",
    "            probabilities[pos] = {\n",
    "                'original_aa': original_aa,\n",
    "                'aa_probabilities': aa_probs\n",
    "            }\n",
    "    \n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Analysis Functions ---\n",
    "\n",
    "def reconstruct_sequence_from_tokens(tokens):\n",
    "    \"\"\"Reconstruct amino acid sequence from tokens.\"\"\"\n",
    "    if isinstance(tokens, str):\n",
    "        tokens = ast.literal_eval(tokens)\n",
    "    return ''.join(tokens)\n",
    "\n",
    "\n",
    "def find_differing_amino_acid_positions(original_seq, mutated_seq, alternative_seq, baseline_seq=None):\n",
    "    \"\"\"Find positions where amino acids differ between sequences.\"\"\"\n",
    "    differing_positions = []\n",
    "    sequences = [mutated_seq, alternative_seq]\n",
    "    if baseline_seq:\n",
    "        sequences.append(baseline_seq)\n",
    "    \n",
    "    min_len = min(len(original_seq), *[len(seq) for seq in sequences])\n",
    "    \n",
    "    for i in range(min_len):\n",
    "        original_aa = original_seq[i]\n",
    "        differs = any(seq[i] != original_aa for seq in sequences)\n",
    "        if differs:\n",
    "            differing_positions.append(i)\n",
    "    \n",
    "    return differing_positions\n",
    "\n",
    "\n",
    "def normalize_embeddings(embeddings, method='l2'):\n",
    "    \"\"\"Normalize embeddings using different methods.\"\"\"\n",
    "    if method == 'l2':\n",
    "        return F.normalize(embeddings, p=2, dim=-1)\n",
    "    elif method == 'center_l2':\n",
    "        centered = embeddings - embeddings.mean(dim=0, keepdim=True)\n",
    "        return F.normalize(centered, p=2, dim=-1)\n",
    "    else:\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "def compute_similarities_multiple_methods(original_emb, comparison_emb, positions, method='concat'):\n",
    "    \"\"\"Compute similarities using different aggregation methods.\"\"\"\n",
    "    if not positions or original_emb is None or comparison_emb is None:\n",
    "        return None\n",
    "    \n",
    "    # Ensure positions are within bounds\n",
    "    max_pos = min(original_emb.shape[0] - 1, comparison_emb.shape[0] - 1)\n",
    "    valid_positions = [pos for pos in positions if pos <= max_pos]\n",
    "    \n",
    "    if not valid_positions:\n",
    "        return None\n",
    "    \n",
    "    if method == 'concat':\n",
    "        # Concatenate embeddings at differing positions\n",
    "        orig_concat = original_emb[valid_positions].flatten().unsqueeze(0).numpy()\n",
    "        comp_concat = comparison_emb[valid_positions].flatten().unsqueeze(0).numpy()\n",
    "        return cosine_similarity(orig_concat, comp_concat)[0, 0]\n",
    "    \n",
    "    elif method == 'position_wise':\n",
    "        # Compute position-wise similarities and average\n",
    "        similarities = []\n",
    "        for pos in valid_positions:\n",
    "            orig_vec = original_emb[pos].unsqueeze(0).numpy()\n",
    "            comp_vec = comparison_emb[pos].unsqueeze(0).numpy()\n",
    "            sim = cosine_similarity(orig_vec, comp_vec)[0, 0]\n",
    "            similarities.append(sim)\n",
    "        return np.mean(similarities)\n",
    "    \n",
    "    elif method == 'global_mean':\n",
    "        # Global mean pooling over entire sequence\n",
    "        orig_mean = original_emb.mean(dim=0).unsqueeze(0).numpy()\n",
    "        comp_mean = comparison_emb.mean(dim=0).unsqueeze(0).numpy()\n",
    "        return cosine_similarity(orig_mean, comp_mean)[0, 0]\n",
    "    \n",
    "    elif method == 'local_window':\n",
    "        # Local window around each changed position\n",
    "        window_size = 8\n",
    "        similarities = []\n",
    "        for pos in valid_positions:\n",
    "            start = max(0, pos - window_size)\n",
    "            end = min(original_emb.shape[0], pos + window_size + 1)\n",
    "            \n",
    "            orig_window = original_emb[start:end].mean(dim=0).unsqueeze(0).numpy()\n",
    "            comp_window = comparison_emb[start:end].mean(dim=0).unsqueeze(0).numpy()\n",
    "            sim = cosine_similarity(orig_window, comp_window)[0, 0]\n",
    "            similarities.append(sim)\n",
    "        return np.mean(similarities)\n",
    "\n",
    "\n",
    "def compute_probability_metrics(original_seq, comparison_seq, differing_positions, sequence_data, tokenizer):\n",
    "    \"\"\"Compute probability-based metrics.\"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    # Get logits for both sequences\n",
    "    orig_logits = sequence_data.get(original_seq, {}).get('logits')\n",
    "    comp_logits = sequence_data.get(comparison_seq, {}).get('logits')\n",
    "    \n",
    "    if orig_logits is None or comp_logits is None:\n",
    "        return metrics\n",
    "    \n",
    "    # Compute pseudo-perplexity difference\n",
    "    orig_log_probs = F.log_softmax(orig_logits, dim=-1)\n",
    "    comp_log_probs = F.log_softmax(comp_logits, dim=-1)\n",
    "    \n",
    "    # Get log probabilities at differing positions\n",
    "    orig_position_probs = []\n",
    "    comp_position_probs = []\n",
    "    \n",
    "    for pos in differing_positions:\n",
    "        if pos < orig_logits.shape[0] and pos < comp_logits.shape[0]:\n",
    "            orig_aa = original_seq[pos]\n",
    "            comp_aa = comparison_seq[pos]\n",
    "            \n",
    "            # Get token IDs\n",
    "            orig_token_id = tokenizer.convert_tokens_to_ids(orig_aa)\n",
    "            comp_token_id = tokenizer.convert_tokens_to_ids(comp_aa)\n",
    "            \n",
    "            if orig_token_id < orig_log_probs.shape[1] and comp_token_id < comp_log_probs.shape[1]:\n",
    "                orig_position_probs.append(orig_log_probs[pos, orig_token_id].item())\n",
    "                comp_position_probs.append(comp_log_probs[pos, comp_token_id].item())\n",
    "    \n",
    "    if orig_position_probs and comp_position_probs:\n",
    "        metrics['avg_log_prob_orig'] = np.mean(orig_position_probs)\n",
    "        metrics['avg_log_prob_comp'] = np.mean(comp_position_probs)\n",
    "        metrics['log_prob_diff'] = np.mean(orig_position_probs) - np.mean(comp_position_probs)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def compute_rank_based_metrics(original_seq, mutated_seq, alternative_seq, baseline_seq, differing_positions, model, tokenizer, device):\n",
    "    \"\"\"Compute rank-based metrics using masked probabilities.\"\"\"\n",
    "    rank_metrics = {'orig_wins_vs_mut': 0, 'orig_wins_vs_alt': 0, 'orig_wins_vs_base': 0, 'mut_wins_vs_alt': 0, 'total_positions': 0}\n",
    "    \n",
    "    if not differing_positions:\n",
    "        return rank_metrics\n",
    "    \n",
    "    masked_probs = compute_masked_probabilities(original_seq, differing_positions, model, tokenizer, device)\n",
    "    \n",
    "    for pos in differing_positions:\n",
    "        if pos not in masked_probs:\n",
    "            continue\n",
    "        \n",
    "        aa_probs = masked_probs[pos]['aa_probabilities']\n",
    "        orig_aa = original_seq[pos]\n",
    "        mut_aa = mutated_seq[pos] if pos < len(mutated_seq) else orig_aa\n",
    "        alt_aa = alternative_seq[pos] if pos < len(alternative_seq) else orig_aa\n",
    "        base_aa = baseline_seq[pos] if (baseline_seq and pos < len(baseline_seq)) else orig_aa\n",
    "        \n",
    "        orig_prob = aa_probs.get(orig_aa, 0)\n",
    "        mut_prob = aa_probs.get(mut_aa, 0)\n",
    "        alt_prob = aa_probs.get(alt_aa, 0)\n",
    "        base_prob = aa_probs.get(base_aa, 0)\n",
    "        \n",
    "        if orig_prob > mut_prob:\n",
    "            rank_metrics['orig_wins_vs_mut'] += 1\n",
    "        if orig_prob > alt_prob:\n",
    "            rank_metrics['orig_wins_vs_alt'] += 1\n",
    "        if orig_prob > base_prob:\n",
    "            rank_metrics['orig_wins_vs_base'] += 1\n",
    "        if mut_prob > alt_prob:\n",
    "            rank_metrics['mut_wins_vs_alt'] += 1\n",
    "        \n",
    "        rank_metrics['total_positions'] += 1\n",
    "    \n",
    "    return rank_metrics\n",
    "\n",
    "\n",
    "def get_blosum_scores(original_seq, comparison_seq, differing_positions, sub_matrix):\n",
    "    \"\"\"Get BLOSUM scores for differing positions.\"\"\"\n",
    "    scores = []\n",
    "    for pos in differing_positions:\n",
    "        if pos < len(original_seq) and pos < len(comparison_seq):\n",
    "            orig_aa = original_seq[pos]\n",
    "            comp_aa = comparison_seq[pos]\n",
    "            try:\n",
    "                score = sub_matrix[(orig_aa, comp_aa)]\n",
    "                scores.append(score)\n",
    "            except KeyError:\n",
    "                continue\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Main Enhanced Experiment Function ---\n",
    "\n",
    "def run_enhanced_protein_similarity_experiment(\n",
    "    df_protein_oma, \n",
    "    sub_matrix_precomputed_alternatives,\n",
    "    batch_size=32, \n",
    "    max_length=514, \n",
    "    extract_layers=[-1],\n",
    "    normalization_method='l2',\n",
    "    similarity_methods=['concat', 'position_wise', 'global_mean'],\n",
    "    include_probability_analysis=True,\n",
    "    include_rank_analysis=True,\n",
    "    include_baseline=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Enhanced main function with all improvements integrated.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize ESM-2 model\n",
    "    model, tokenizer, device = get_esm2_model_and_tokenizer()\n",
    "    \n",
    "    # Identify tokenizer columns\n",
    "    tokenizer_columns = list(tokenizer_list.keys())\n",
    "    print(f\"Found {len(tokenizer_columns)} tokenizer columns to process\")\n",
    "    \n",
    "    # Collect all unique sequences\n",
    "    print(\"Collecting unique sequences...\")\n",
    "    unique_sequences = set()\n",
    "    \n",
    "    for tokenizer_col in tokenizer_columns:\n",
    "        mutated_col = tokenizer_col + ' mutated'\n",
    "        alternative_col = tokenizer_col + ' alternative'\n",
    "        baseline_col = tokenizer_col + ' baseline' if include_baseline else None\n",
    "        \n",
    "        for idx, row in df_protein_oma.iterrows():\n",
    "            try:\n",
    "                original_tokens = row[tokenizer_col]\n",
    "                mutated_tokens = row[mutated_col]\n",
    "                alternative_tokens = row[alternative_col]\n",
    "                \n",
    "                original_seq = reconstruct_sequence_from_tokens(original_tokens)\n",
    "                mutated_seq = reconstruct_sequence_from_tokens(mutated_tokens)\n",
    "                alternative_seq = reconstruct_sequence_from_tokens(alternative_tokens)\n",
    "                \n",
    "                unique_sequences.update([original_seq, mutated_seq, alternative_seq])\n",
    "                \n",
    "                if include_baseline and baseline_col in row:\n",
    "                    baseline_tokens = row[baseline_col]\n",
    "                    baseline_seq = reconstruct_sequence_from_tokens(baseline_tokens)\n",
    "                    unique_sequences.add(baseline_seq)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                continue\n",
    "    \n",
    "    unique_sequences = list(unique_sequences)\n",
    "    print(f\"Found {len(unique_sequences)} unique sequences\")\n",
    "    \n",
    "    # Get embeddings and logits for all sequences\n",
    "    print(\"Computing ESM-2 embeddings and logits...\")\n",
    "    sequence_data = get_esm2_embeddings_and_logits_batch(\n",
    "        unique_sequences, model, tokenizer, device, batch_size, max_length, extract_layers\n",
    "    )\n",
    "    \n",
    "    # Clear GPU memory\n",
    "    torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "    \n",
    "    # Process each tokenizer\n",
    "    all_results = []\n",
    "    \n",
    "    for tokenizer_col in tokenizer_columns:\n",
    "        print(f\"\\nProcessing tokenizer: {tokenizer_col}\")\n",
    "        \n",
    "        # Get substitution matrix for BLOSUM analysis\n",
    "        sub_matrix_name = tokenizer_col.split()[1]\n",
    "        sub_matrix = substitution_matrices.load(sub_matrix_name.upper())\n",
    "        \n",
    "        mutated_col = tokenizer_col + ' mutated'\n",
    "        alternative_col = tokenizer_col + ' alternative'\n",
    "        baseline_col = tokenizer_col + ' baseline' if include_baseline else None\n",
    "        \n",
    "        # Initialize result containers\n",
    "        results = {\n",
    "            'similarities': {method: {'mut': [], 'alt': [], 'base': []} for method in similarity_methods},\n",
    "            'probability_metrics': {'mut': [], 'alt': [], 'base': []},\n",
    "            'rank_metrics': [],\n",
    "            'blosum_scores': {'mut': [], 'alt': [], 'base': []},\n",
    "            'position_counts': []\n",
    "        }\n",
    "        \n",
    "        processed_proteins = 0\n",
    "        \n",
    "        for idx, row in tqdm(df_protein_oma.iterrows(), total=len(df_protein_oma), desc=\"Processing proteins\"):\n",
    "            try:\n",
    "                # Reconstruct sequences\n",
    "                original_seq = reconstruct_sequence_from_tokens(row[tokenizer_col])\n",
    "                mutated_seq = reconstruct_sequence_from_tokens(row[mutated_col])\n",
    "                alternative_seq = reconstruct_sequence_from_tokens(row[alternative_col])\n",
    "                baseline_seq = None\n",
    "                \n",
    "                if include_baseline and baseline_col in row:\n",
    "                    baseline_seq = reconstruct_sequence_from_tokens(row[baseline_col])\n",
    "                \n",
    "                # Skip if no changes\n",
    "                if (original_seq == mutated_seq and original_seq == alternative_seq and \n",
    "                    (not include_baseline or original_seq == baseline_seq)):\n",
    "                    continue\n",
    "                \n",
    "                # Find differing positions\n",
    "                differing_positions = find_differing_amino_acid_positions(\n",
    "                    original_seq, mutated_seq, alternative_seq, baseline_seq\n",
    "                )\n",
    "                \n",
    "                if not differing_positions:\n",
    "                    continue\n",
    "                \n",
    "                results['position_counts'].append(len(differing_positions))\n",
    "                \n",
    "                # Get sequence data\n",
    "                orig_data = sequence_data.get(original_seq, {})\n",
    "                mut_data = sequence_data.get(mutated_seq, {})\n",
    "                alt_data = sequence_data.get(alternative_seq, {})\n",
    "                base_data = sequence_data.get(baseline_seq, {}) if baseline_seq else {}\n",
    "                \n",
    "                # Process each layer\n",
    "                for layer_idx in extract_layers:\n",
    "                    orig_emb = orig_data.get('embeddings', {}).get(layer_idx)\n",
    "                    mut_emb = mut_data.get('embeddings', {}).get(layer_idx)\n",
    "                    alt_emb = alt_data.get('embeddings', {}).get(layer_idx)\n",
    "                    base_emb = base_data.get('embeddings', {}).get(layer_idx) if baseline_seq else None\n",
    "                    \n",
    "                    if orig_emb is None:\n",
    "                        continue\n",
    "                    \n",
    "                    # Normalize embeddings\n",
    "                    orig_emb_norm = normalize_embeddings(orig_emb, normalization_method)\n",
    "                    mut_emb_norm = normalize_embeddings(mut_emb, normalization_method) if mut_emb is not None else None\n",
    "                    alt_emb_norm = normalize_embeddings(alt_emb, normalization_method) if alt_emb is not None else None\n",
    "                    base_emb_norm = normalize_embeddings(base_emb, normalization_method) if base_emb is not None else None\n",
    "                    \n",
    "                    # Compute similarities using different methods\n",
    "                    for method in similarity_methods:\n",
    "                        if mut_emb_norm is not None:\n",
    "                            sim_mut = compute_similarities_multiple_methods(\n",
    "                                orig_emb_norm, mut_emb_norm, differing_positions, method\n",
    "                            )\n",
    "                            if sim_mut is not None:\n",
    "                                results['similarities'][method]['mut'].append(sim_mut)\n",
    "                        \n",
    "                        if alt_emb_norm is not None:\n",
    "                            sim_alt = compute_similarities_multiple_methods(\n",
    "                                orig_emb_norm, alt_emb_norm, differing_positions, method\n",
    "                            )\n",
    "                            if sim_alt is not None:\n",
    "                                results['similarities'][method]['alt'].append(sim_alt)\n",
    "                        \n",
    "                        if base_emb_norm is not None:\n",
    "                            sim_base = compute_similarities_multiple_methods(\n",
    "                                orig_emb_norm, base_emb_norm, differing_positions, method\n",
    "                            )\n",
    "                            if sim_base is not None:\n",
    "                                results['similarities'][method]['base'].append(sim_base)\n",
    "                \n",
    "                # Probability analysis\n",
    "                if include_probability_analysis:\n",
    "                    prob_metrics_mut = compute_probability_metrics(\n",
    "                        original_seq, mutated_seq, differing_positions, sequence_data, tokenizer\n",
    "                    )\n",
    "                    prob_metrics_alt = compute_probability_metrics(\n",
    "                        original_seq, alternative_seq, differing_positions, sequence_data, tokenizer\n",
    "                    )\n",
    "                    \n",
    "                    results['probability_metrics']['mut'].append(prob_metrics_mut)\n",
    "                    results['probability_metrics']['alt'].append(prob_metrics_alt)\n",
    "                    \n",
    "                    if baseline_seq:\n",
    "                        prob_metrics_base = compute_probability_metrics(\n",
    "                            original_seq, baseline_seq, differing_positions, sequence_data, tokenizer\n",
    "                        )\n",
    "                        results['probability_metrics']['base'].append(prob_metrics_base)\n",
    "                \n",
    "                # Rank analysis\n",
    "                if include_rank_analysis:\n",
    "                    rank_metrics = compute_rank_based_metrics(\n",
    "                        original_seq, mutated_seq, alternative_seq, baseline_seq, differing_positions, \n",
    "                        model, tokenizer, device\n",
    "                    )\n",
    "                    results['rank_metrics'].append(rank_metrics)\n",
    "                \n",
    "                # BLOSUM score analysis\n",
    "                blosum_mut = get_blosum_scores(original_seq, mutated_seq, differing_positions, sub_matrix)\n",
    "                blosum_alt = get_blosum_scores(original_seq, alternative_seq, differing_positions, sub_matrix)\n",
    "                \n",
    "                results['blosum_scores']['mut'].extend(blosum_mut)\n",
    "                results['blosum_scores']['alt'].extend(blosum_alt)\n",
    "                \n",
    "                if baseline_seq:\n",
    "                    blosum_base = get_blosum_scores(original_seq, baseline_seq, differing_positions, sub_matrix)\n",
    "                    results['blosum_scores']['base'].extend(blosum_base)\n",
    "                \n",
    "                processed_proteins += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing row {idx}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Compile results for this tokenizer\n",
    "        tokenizer_results = {\n",
    "            'tokenizer': tokenizer_col,\n",
    "            'processed_proteins': processed_proteins,\n",
    "            'total_differing_positions': sum(results['position_counts']),\n",
    "            'avg_differing_positions': np.mean(results['position_counts']) if results['position_counts'] else 0\n",
    "        }\n",
    "        \n",
    "        # Add similarity results\n",
    "        for method in similarity_methods:\n",
    "            for comparison in ['mut', 'alt', 'base']:\n",
    "                if results['similarities'][method][comparison]:\n",
    "                    tokenizer_results[f'{method}_similarity_{comparison}'] = np.mean(results['similarities'][method][comparison])\n",
    "        \n",
    "        # Add rank results\n",
    "        if results['rank_metrics']:\n",
    "            total_wins_mut = sum(r['orig_wins_vs_mut'] for r in results['rank_metrics'])\n",
    "            total_wins_alt = sum(r['orig_wins_vs_alt'] for r in results['rank_metrics'])\n",
    "            total_wins_base = sum(r['orig_wins_vs_base'] for r in results['rank_metrics'])\n",
    "            total_wins_mutaalt = sum(r['mut_wins_vs_alt'] for r in results['rank_metrics'])\n",
    "            total_positions = sum(r['total_positions'] for r in results['rank_metrics'])\n",
    "            \n",
    "            if total_positions > 0:\n",
    "                tokenizer_results['rank_win_rate_vs_mut'] = total_wins_mut / total_positions\n",
    "                tokenizer_results['rank_win_rate_vs_alt'] = total_wins_alt / total_positions\n",
    "                tokenizer_results['rank_win_rate_vs_base'] = total_wins_base / total_positions\n",
    "                tokenizer_results['rank_win_rate_mut_vs_alt'] = total_wins_mutaalt / total_positions\n",
    "        \n",
    "        # Add BLOSUM correlation analysis\n",
    "        for comparison in ['mut', 'alt', 'base']:\n",
    "            blosum_scores = results['blosum_scores'][comparison]\n",
    "            similarities = results['similarities']['concat'][comparison]  # Use concat method for correlation\n",
    "            \n",
    "            if len(blosum_scores) > 0 and len(similarities) > 0 and len(blosum_scores) == len(similarities):\n",
    "                correlation, p_value = stats.pearsonr(blosum_scores, similarities)\n",
    "                tokenizer_results[f'blosum_similarity_correlation_{comparison}'] = correlation\n",
    "                tokenizer_results[f'blosum_similarity_p_value_{comparison}'] = p_value\n",
    "        \n",
    "        all_results.append(tokenizer_results)\n",
    "    \n",
    "    return pd.DataFrame(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting enhanced protein mutation analysis...\n",
      "Pre-computing alternative amino acid scores...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:00<00:00, 835.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-computing alternative amino acid scores...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:00<00:00, 809.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-computing alternative amino acid scores...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:00<00:00, 1137.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-computing alternative amino acid scores...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:00<00:00, 1212.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running mutation experiment with baseline...\n",
      "\n",
      "Processing column: PUMA blosum62 0.7 0.05 51200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 58149/58149 [00:12<00:00, 4535.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing column: PUMA blosum62 0.7 0.005 51200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 58149/58149 [00:14<00:00, 4067.80it/s]\n"
     ]
    }
   ],
   "source": [
    "# Initialize and run the experiment\n",
    "print(\"Starting enhanced protein mutation analysis...\")\n",
    "\n",
    "# Precompute alternatives for all substitution matrices\n",
    "sub_matrix_precomputed_alternatives = {}\n",
    "for sub_matrix_name in ['blosum45', 'blosum62', 'pam70', 'pam250']:\n",
    "    sub_matrix = substitution_matrices.load(sub_matrix_name.upper())\n",
    "    sub_matrix_precomputed_alternatives[sub_matrix_name] = precompute_alternatives(sub_matrix)\n",
    "\n",
    "# Run mutation experiment with baseline\n",
    "print(\"Running mutation experiment with baseline...\")\n",
    "df_protein_oma, change_counters = run_mutation_experiment_optimized(\n",
    "    df_protein, vocab_lineage_list, sub_matrix_precomputed_alternatives, create_baseline=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running enhanced similarity experiment...\n",
      "Using device: cuda\n",
      "Loaded ESM-2 model: facebook/esm2_t30_150M_UR50D\n",
      "Found 2 tokenizer columns to process\n",
      "Collecting unique sequences...\n",
      "Found 689 unique sequences\n",
      "Computing ESM-2 embeddings and logits...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing embedding batches: 100%|██████████| 11/11 [00:04<00:00,  2.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing tokenizer: PUMA blosum62 0.7 0.05 51200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing proteins: 100%|██████████| 100/100 [00:20<00:00,  4.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing tokenizer: PUMA blosum62 0.7 0.005 51200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing proteins: 100%|██████████| 100/100 [00:22<00:00,  4.51it/s]\n"
     ]
    }
   ],
   "source": [
    "# Run enhanced similarity experiment\n",
    "print(\"Running enhanced similarity experiment...\")\n",
    "results_df = run_enhanced_protein_similarity_experiment(\n",
    "    df_protein_oma[:100],\n",
    "    sub_matrix_precomputed_alternatives,\n",
    "    batch_size=64,\n",
    "    extract_layers=[6, 12, -1],  # Multiple layers\n",
    "    similarity_methods=['concat', 'position_wise', 'global_mean', 'local_window'],\n",
    "    include_probability_analysis=False,\n",
    "    include_rank_analysis=True,\n",
    "    include_baseline=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>processed_proteins</th>\n",
       "      <th>total_differing_positions</th>\n",
       "      <th>avg_differing_positions</th>\n",
       "      <th>concat_similarity_mut</th>\n",
       "      <th>concat_similarity_alt</th>\n",
       "      <th>concat_similarity_base</th>\n",
       "      <th>position_wise_similarity_mut</th>\n",
       "      <th>position_wise_similarity_alt</th>\n",
       "      <th>position_wise_similarity_base</th>\n",
       "      <th>global_mean_similarity_mut</th>\n",
       "      <th>global_mean_similarity_alt</th>\n",
       "      <th>global_mean_similarity_base</th>\n",
       "      <th>local_window_similarity_mut</th>\n",
       "      <th>local_window_similarity_alt</th>\n",
       "      <th>local_window_similarity_base</th>\n",
       "      <th>rank_win_rate_vs_mut</th>\n",
       "      <th>rank_win_rate_vs_alt</th>\n",
       "      <th>rank_win_rate_vs_base</th>\n",
       "      <th>rank_win_rate_mut_vs_alt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PUMA blosum62 0.7 0.05 51200</td>\n",
       "      <td>98</td>\n",
       "      <td>1014</td>\n",
       "      <td>10.346939</td>\n",
       "      <td>0.927336</td>\n",
       "      <td>0.943783</td>\n",
       "      <td>0.929539</td>\n",
       "      <td>0.927336</td>\n",
       "      <td>0.943783</td>\n",
       "      <td>0.929539</td>\n",
       "      <td>0.998688</td>\n",
       "      <td>0.998840</td>\n",
       "      <td>0.998875</td>\n",
       "      <td>0.994957</td>\n",
       "      <td>0.996405</td>\n",
       "      <td>0.995617</td>\n",
       "      <td>0.905325</td>\n",
       "      <td>0.979290</td>\n",
       "      <td>0.883629</td>\n",
       "      <td>0.852071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PUMA blosum62 0.7 0.005 51200</td>\n",
       "      <td>99</td>\n",
       "      <td>1095</td>\n",
       "      <td>11.060606</td>\n",
       "      <td>0.928825</td>\n",
       "      <td>0.945729</td>\n",
       "      <td>0.931397</td>\n",
       "      <td>0.928825</td>\n",
       "      <td>0.945729</td>\n",
       "      <td>0.931397</td>\n",
       "      <td>0.998657</td>\n",
       "      <td>0.998866</td>\n",
       "      <td>0.999026</td>\n",
       "      <td>0.994920</td>\n",
       "      <td>0.996618</td>\n",
       "      <td>0.995743</td>\n",
       "      <td>0.927854</td>\n",
       "      <td>0.911416</td>\n",
       "      <td>0.835616</td>\n",
       "      <td>0.754338</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       tokenizer  processed_proteins  \\\n",
       "0   PUMA blosum62 0.7 0.05 51200                  98   \n",
       "1  PUMA blosum62 0.7 0.005 51200                  99   \n",
       "\n",
       "   total_differing_positions  avg_differing_positions  concat_similarity_mut  \\\n",
       "0                       1014                10.346939               0.927336   \n",
       "1                       1095                11.060606               0.928825   \n",
       "\n",
       "   concat_similarity_alt  concat_similarity_base  \\\n",
       "0               0.943783                0.929539   \n",
       "1               0.945729                0.931397   \n",
       "\n",
       "   position_wise_similarity_mut  position_wise_similarity_alt  \\\n",
       "0                      0.927336                      0.943783   \n",
       "1                      0.928825                      0.945729   \n",
       "\n",
       "   position_wise_similarity_base  global_mean_similarity_mut  \\\n",
       "0                       0.929539                    0.998688   \n",
       "1                       0.931397                    0.998657   \n",
       "\n",
       "   global_mean_similarity_alt  global_mean_similarity_base  \\\n",
       "0                    0.998840                     0.998875   \n",
       "1                    0.998866                     0.999026   \n",
       "\n",
       "   local_window_similarity_mut  local_window_similarity_alt  \\\n",
       "0                     0.994957                     0.996405   \n",
       "1                     0.994920                     0.996618   \n",
       "\n",
       "   local_window_similarity_base  rank_win_rate_vs_mut  rank_win_rate_vs_alt  \\\n",
       "0                      0.995617              0.905325              0.979290   \n",
       "1                      0.995743              0.927854              0.911416   \n",
       "\n",
       "   rank_win_rate_vs_base  rank_win_rate_mut_vs_alt  \n",
       "0               0.883629                  0.852071  \n",
       "1               0.835616                  0.754338  "
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>processed_proteins</th>\n",
       "      <th>rank_win_rate_vs_mut</th>\n",
       "      <th>rank_win_rate_vs_alt</th>\n",
       "      <th>rank_win_rate_vs_base</th>\n",
       "      <th>rank_win_rate_mut_vs_alt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PUMA blosum62 0.7 0.05 51200</td>\n",
       "      <td>98</td>\n",
       "      <td>0.905325</td>\n",
       "      <td>0.979290</td>\n",
       "      <td>0.883629</td>\n",
       "      <td>0.852071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PUMA blosum62 0.7 0.005 51200</td>\n",
       "      <td>99</td>\n",
       "      <td>0.927854</td>\n",
       "      <td>0.911416</td>\n",
       "      <td>0.835616</td>\n",
       "      <td>0.754338</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       tokenizer  processed_proteins  rank_win_rate_vs_mut  \\\n",
       "0   PUMA blosum62 0.7 0.05 51200                  98              0.905325   \n",
       "1  PUMA blosum62 0.7 0.005 51200                  99              0.927854   \n",
       "\n",
       "   rank_win_rate_vs_alt  rank_win_rate_vs_base  rank_win_rate_mut_vs_alt  \n",
       "0              0.979290               0.883629                  0.852071  \n",
       "1              0.911416               0.835616                  0.754338  "
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df[results_df.columns[[0,1,-4,-3,-2,-1]]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
