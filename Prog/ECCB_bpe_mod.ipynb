{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4412519-729d-48be-850c-5603acb0c981",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "\n",
    "from Bio import Align\n",
    "from Bio.Align import substitution_matrices\n",
    "\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4e54654-eb75-44b7-bc1f-877403d077ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_aligner(mode='global', sub_matrix='BLOSUM62', open_gap_score=-11, extend_gap_score=-1):\n",
    "    '''\n",
    "    aligner = Align.PairwiseAligner(mode='global', open_gap_score=-9, extend_gap_score=-1, substitution_matrix=substitution_matrices.load(\"PAM30\"))\n",
    "    aligner = Align.PairwiseAligner(mode='global', open_gap_score=-10, extend_gap_score=-1, substitution_matrix=substitution_matrices.load(\"PAM70\"))\n",
    "    aligner = Align.PairwiseAligner(mode='global', open_gap_score=-14, extend_gap_score=-2, substitution_matrix=substitution_matrices.load(\"PAM250\"))\n",
    "    aligner = Align.PairwiseAligner(mode='global', open_gap_score=-10, extend_gap_score=-1, substitution_matrix=substitution_matrices.load(\"BLOSUM80\"))\n",
    "    aligner = Align.PairwiseAligner(mode='global', open_gap_score=-11, extend_gap_score=-1, substitution_matrix=substitution_matrices.load(\"BLOSUM62\"))\n",
    "    aligner = Align.PairwiseAligner(mode='global', open_gap_score=-15, extend_gap_score=-2, substitution_matrix=substitution_matrices.load(\"BLOSUM45\"))\n",
    "    aligner = Align.PairwiseAligner(mode='global', open_gap_score=-13, extend_gap_score=-2, substitution_matrix=substitution_matrices.load(\"BLOSUM50\"))\n",
    "    aligner = Align.PairwiseAligner(mode='global', open_gap_score=-10, extend_gap_score=-1, substitution_matrix=substitution_matrices.load(\"BLOSUM90\"))\n",
    "    '''\n",
    "    return Align.PairwiseAligner(mode=mode, open_gap_score=open_gap_score, extend_gap_score=extend_gap_score, substitution_matrix=substitution_matrices.load(sub_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "787c04a3-1c59-41dc-8d9e-8caaa2fb9284",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pair_scoring(token1, token2, aligner):\n",
    "    if (token1, token2) in alignment_scores_cache:\n",
    "        return alignment_scores_cache[(token1, token2)]\n",
    "    else:\n",
    "        score = aligner.score(token1, token2)\n",
    "        alignment_scores_cache[(token1, token2)] = score\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b10cb52b-06c4-4d05-bc3e-043d96d2ba75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def genetic_aa_mutation2(ref_pair, alphabet, pair_freqs, mutation_perc, sample_count):\n",
    "    '''\n",
    "    genetic_aa_mutation(ref_pair=('AB','CD'), alphabet=alphabet, pair_freqs=pair_freqs, mutation_perc=.33, sample_count=2)\n",
    "    [(('AB', 'CN'), 0), (('IX', 'CD'), 0)]\n",
    "    '''\n",
    "    \n",
    "    samples = []\n",
    "    for _ in range(sample_count*2):\n",
    "        new_sample = list(ref_pair[0]+ref_pair[1])\n",
    "        for i in range(len(new_sample)):\n",
    "            if np.random.rand() < mutation_perc:\n",
    "                alph = list(aligner.alphabet[:-4])\n",
    "                new_sample[i] = alph[np.random.randint(len(alph))]\n",
    "        new_sample = ''.join(new_sample)\n",
    "        if new_sample == ''.join(ref_pair):\n",
    "            continue\n",
    "        new_sample_pair_1 = new_sample[:len(ref_pair[0])]\n",
    "        new_sample_pair_2 = new_sample[len(ref_pair[0]):]\n",
    "        samples.append(((new_sample_pair_1, new_sample_pair_2), pair_freqs[(new_sample_pair_1, new_sample_pair_2)]))\n",
    "    return list(set(samples))[:sample_count]\n",
    "\n",
    "def genetic_aa_mutation(ref_pair, alphabet, pair_freqs, mutation_perc, sample_count):\n",
    "    '''\n",
    "    genetic_aa_mutation(ref_pair=('AB','CD'), alphabet=alphabet, pair_freqs=pair_freqs, mutation_perc=.33, sample_count=2)\n",
    "    [(('AB', 'CN'), 0), (('IX', 'CD'), 0)]\n",
    "    '''\n",
    "    \n",
    "    samples = []\n",
    "    for _ in range(sample_count*2):\n",
    "        new_sample = list(ref_pair[0]+ref_pair[1])\n",
    "        for i in range(len(new_sample)):\n",
    "            if np.random.rand() < mutation_perc:\n",
    "                subs = aligner.substitution_matrix[new_sample[i]][:-4]\n",
    "                subs = subs+abs(min(subs))+1\n",
    "                new_sample[i] = np.random.choice(list(aligner.alphabet[:-4]), p=subs/sum(subs))\n",
    "        new_sample = ''.join(new_sample)\n",
    "        if new_sample == ''.join(ref_pair):\n",
    "            continue\n",
    "        new_sample_pair_1 = new_sample[:len(ref_pair[0])]\n",
    "        new_sample_pair_2 = new_sample[len(ref_pair[0]):]\n",
    "        samples.append(((new_sample_pair_1, new_sample_pair_2), pair_freqs[(new_sample_pair_1, new_sample_pair_2)]))\n",
    "    return list(set(samples))[:sample_count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79a0cb5f-8c6c-4002-b190-9aefc090c5af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def choice_of_freqs(pair_freqs_sorted_list, sample_count, search_size=None):\n",
    "    '''\n",
    "    choice_of_freqs(pair_freqs_sorted_list, sample_count=2)\n",
    "    [(('Y', 'Y'), 12), (('C', 'YN'), 10)]\n",
    "    '''\n",
    "    sample_count = min(sample_count, len(pair_freqs_sorted_list))\n",
    "    index_list = list(range(len(pair_freqs_sorted_list)))[:search_size]\n",
    "    freqs_list = [pf[1] for pf in pair_freqs_sorted_list][:search_size]\n",
    "    if len(freqs_list) == 0:\n",
    "        return []\n",
    "    index_choices = np.random.choice(index_list, size=sample_count, replace=False, p=freqs_list/np.sum(freqs_list))\n",
    "    return [pair_freqs_sorted_list[ic] for ic in index_choices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae1c55f2-eb4a-4fa3-8bc8-575aee843d23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_alignment_scores(ref_pair, comp_pairs_list, aligner):\n",
    "    '''\n",
    "    calculate_alignment_scores((('F', 'Y'), 12), [(('Y', 'Y'), 12), (('C', 'YN'), 10)])\n",
    "    [((('F', 'Y'), 12), (('Y', 'Y'), 12), 10.0, 240.0),\n",
    "    ((('F', 'Y'), 12), (('C', 'YN'), 10), -6.0, -132.0)]\n",
    "    '''\n",
    "    alignment_scores = [(ref_pair, comp_pair, pair_scoring(''.join(ref_pair[0]), ''.join(comp_pair[0]), aligner),) for comp_pair in comp_pairs_list]\n",
    "    alignment_scores = [(*alignment_score, (np.log(ref_pair[1])+np.log(alignment_score[1][1]))*alignment_score[2]) for alignment_score in alignment_scores]\n",
    "    return alignment_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "efa7a6d8-2e1e-404b-8516-1033f0f38024",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_alignment_scores_gen(ref_pair, comp_pairs_list, aligner):\n",
    "    '''\n",
    "    calculate_alignment_scores((('F', 'Y'), 12), [(('Y', 'Y'), 12), (('F', 'C'), 10)])\n",
    "    [((('F', 'Y'), 12), (('Y', 'Y'), 12), 10.0, 240.0),\n",
    "    ((('F', 'Y'), 12), (('F', 'C'), 10), 4.0, 88.0)]\n",
    "    '''\n",
    "    ref_comp_list = []\n",
    "    for comp_pair in comp_pairs_list:\n",
    "        diff_ref = ''\n",
    "        diff_comp = ''\n",
    "        for ref_aa, comp_aa in zip(''.join(ref_pair[0]), ''.join(comp_pair[0])):\n",
    "            if ref_aa != comp_aa:\n",
    "                diff_ref += ref_aa\n",
    "                diff_comp += comp_aa\n",
    "        ref_comp_list.append((ref_pair, comp_pair, diff_ref, diff_comp))\n",
    "\n",
    "    alignment_scores = [(ref_pair, comp_pair, pair_scoring(diff_ref, diff_comp, aligner),) for ref_pair, comp_pair, diff_ref, diff_comp in ref_comp_list]\n",
    "    alignment_scores = [(*alignment_score, (np.log(ref_pair[1])+np.log(alignment_score[1][1]))*alignment_score[2]) for alignment_score in alignment_scores]\n",
    "    return alignment_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4397898f-8426-4f00-9c52-c25096f40ebf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_pair_freqs(splits):\n",
    "    pair_freqs = defaultdict(int)\n",
    "    for word, freq in word_freqs.items():\n",
    "        split = splits[word]\n",
    "        if len(split) == 1:\n",
    "            continue\n",
    "        for i in range(len(split) - 1):\n",
    "            pair = (split[i], split[i + 1])\n",
    "            pair_freqs[pair] += freq\n",
    "    return pair_freqs\n",
    "\n",
    "def merge_pair(a, b, splits):\n",
    "    for word in word_freqs:\n",
    "        split = splits[word]\n",
    "        if len(split) == 1:\n",
    "            continue\n",
    "\n",
    "        i = 0\n",
    "        while i < len(split) - 1:\n",
    "            if split[i] == a and split[i + 1] == b:\n",
    "                split = split[:i] + [a + b] + split[i + 2 :]\n",
    "            else:\n",
    "                i += 1\n",
    "        splits[word] = split\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "edb80842-344f-4645-9e20-659b4edccc8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('uniref_taxonomy_id_9606_AND_identity_2024_09_13.json') as f:\n",
    "# with open('../../RSRC/ECCB/uniref_taxonomy_id_9606_AND_identity_2024_09_13.json') as f:\n",
    "    human_proteins_json = json.load(f)['results']\n",
    "    \n",
    "human_proteins_df = []\n",
    "for prot in human_proteins_json:\n",
    "    human_proteins_df.append({'id': prot['id'], 'sequence': prot['representativeMember']['sequence']['value']})\n",
    "human_proteins_df = pd.DataFrame(human_proteins_df)\n",
    "human_proteins_df = human_proteins_df[~human_proteins_df['sequence'].str.contains('U')]\n",
    "\n",
    "df_ds_train, df_ds_test = train_test_split(human_proteins_df, test_size=0.2, random_state=42)\n",
    "\n",
    "corpus = df_ds_train['sequence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c953db8b-26d1-4e8c-a68f-468dbe76ff07",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'X', 'Y']\n"
     ]
    }
   ],
   "source": [
    "word_freqs = defaultdict(int)\n",
    "\n",
    "for text in corpus:\n",
    "    word_freqs[text] += 1\n",
    "    \n",
    "alphabet = []\n",
    "\n",
    "for word in word_freqs.keys():\n",
    "    for letter in word:\n",
    "        if letter not in alphabet:\n",
    "            alphabet.append(letter)\n",
    "alphabet.sort()\n",
    "\n",
    "print(alphabet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a51b51f4-f605-40fc-93ad-799df9d285f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50_genetic_freq_align_100_1_0.33_3_BLOSUM62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 29/29 [01:40<00:00,  3.45s/it]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "vocab_size = 50\n",
    "merges = {}\n",
    "vocab = alphabet.copy()\n",
    "splits = {word: list(word) for word in word_freqs.keys()}\n",
    "\n",
    "method_min_thr = 3\n",
    "mutation_perc = .33\n",
    "sample_count = 100\n",
    "extra_merge_count = 1\n",
    "mode='global'\n",
    "sub_matrix='BLOSUM62'\n",
    "open_gap_score=-11\n",
    "extend_gap_score=-1\n",
    "candidate_method = 'genetic' # 'genetic', 'choice', 'freqs'\n",
    "scoring_method = 'align' # 'align', 'freq_align'\n",
    "\n",
    "print(f'{vocab_size}_{candidate_method}_{scoring_method}_{sample_count}_{extra_merge_count}_{mutation_perc}_{method_min_thr}_{sub_matrix}')\n",
    "\n",
    "alignment_scores_cache = defaultdict(int)\n",
    "aligner = load_aligner(mode=mode, sub_matrix=sub_matrix, open_gap_score=open_gap_score, extend_gap_score=extend_gap_score)\n",
    "\n",
    "with tqdm(total=vocab_size-len(vocab)) as pbar:\n",
    "    while len(vocab) < vocab_size:\n",
    "        t0 = time.time()\n",
    "        pair_freqs = compute_pair_freqs(splits)\n",
    "\n",
    "        pair_freqs_sorted_list = sorted(list(pair_freqs.items()), key=lambda x: (-x[1], (-x[1], len(x[0][0]+x[0][1]))))\n",
    "        best_pair = pair_freqs_sorted_list[0]\n",
    "\n",
    "        if len(best_pair[0][0])+len(best_pair[0][1]) >= method_min_thr:\n",
    "            if candidate_method == 'genetic':\n",
    "                candidate_pairs = genetic_aa_mutation(best_pair[0], alphabet, pair_freqs, mutation_perc, sample_count)\n",
    "            elif candidate_method == 'choice':\n",
    "                candidate_pairs = choice_of_freqs(pair_freqs_sorted_list[1:], sample_count)\n",
    "            else:\n",
    "                candidate_pairs = pair_freqs_sorted_list[1:1+sample_count]\n",
    "\n",
    "            candidate_pairs = [candidate_pair for candidate_pair in candidate_pairs if candidate_pair[1]>0]\n",
    "\n",
    "            if candidate_method == 'genetic':\n",
    "                alignment_scores = calculate_alignment_scores_gen(best_pair, candidate_pairs, aligner)\n",
    "            else:\n",
    "                alignment_scores = calculate_alignment_scores(best_pair, candidate_pairs, aligner)\n",
    "\n",
    "            alignment_scores = [alignment_score for alignment_score in alignment_scores if alignment_score[2]>0]\n",
    "\n",
    "            if scoring_method == 'align':\n",
    "                alignment_scores = sorted(alignment_scores, key=lambda x: (-x[2], -x[3])) # alignment\n",
    "            else:\n",
    "                alignment_scores = sorted(alignment_scores, key=lambda x: (-x[3], -x[2])) # frequency * alignment\n",
    "\n",
    "            new_pairs = [alignment_score[1][0] for alignment_score in alignment_scores[:extra_merge_count]]\n",
    "            new_pairs.insert(0, best_pair[0])\n",
    "        else:\n",
    "            new_pairs = [best_pair[0]]\n",
    "\n",
    "        for new_pair in new_pairs:\n",
    "            splits = merge_pair(*new_pair, splits)\n",
    "            merges[new_pair] = ''.join(new_pair)\n",
    "            vocab.append(''.join(new_pair))\n",
    "\n",
    "        if len(vocab) % 1000 in [25]:\n",
    "            with open(f'vocab_bpe_human_{len(vocab)}_{candidate_method}_{scoring_method}_{sample_count}_{extra_merge_count}_{mutation_perc}_{method_min_thr}_{sub_matrix}.pickle', 'wb') as f:\n",
    "                pickle.dump(vocab, f, pickle.HIGHEST_PROTOCOL)\n",
    "            with open(f'merges_bpe_human_{len(vocab)}_{candidate_method}_{scoring_method}_{sample_count}_{extra_merge_count}_{mutation_perc}_{method_min_thr}_{sub_matrix}.pickle', 'wb') as f:\n",
    "                pickle.dump(merges, f, pickle.HIGHEST_PROTOCOL)\n",
    "            \n",
    "        pbar.update(len(new_pairs))\n",
    "        \n",
    "        \n",
    "with open(f'vocab_bpe_human_{vocab_size}_{candidate_method}_{scoring_method}_{sample_count}_{extra_merge_count}_{mutation_perc}_{method_min_thr}_{sub_matrix}.pickle', 'wb') as f:\n",
    "    pickle.dump(vocab, f, pickle.HIGHEST_PROTOCOL)\n",
    "with open(f'merges_bpe_human_{vocab_size}_{candidate_method}_{scoring_method}_{sample_count}_{extra_merge_count}_{mutation_perc}_{method_min_thr}_{sub_matrix}.pickle', 'wb') as f:\n",
    "    pickle.dump(merges, f, pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erc_env",
   "language": "python",
   "name": "erc_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
